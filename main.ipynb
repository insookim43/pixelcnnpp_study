{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following codes from : https://github.com/pclucas14/pixel-cnn-pp/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import * \n",
    "from model import * \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "important default arguments\n",
    "--data_dir = 'data'\n",
    "--save_dir = 'models'\n",
    "--dataset = 'cifar'\n",
    "--nr_resnet = 5\n",
    "--nr_filters = 160\n",
    "--nr_logistic_mix = 10\n",
    "\n",
    "\n",
    "--lr = 0.0002\n",
    "--lr_decay = 0.999995\n",
    "--batch_size = 64\n",
    "--max_epochs = 5000\n",
    "--seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# data I/O\n",
    "parser.add_argument('-i', '--data_dir', type=str,\n",
    "                    default='data', help='Location for the dataset')\n",
    "parser.add_argument('-o', '--save_dir', type=str, default='models',\n",
    "                    help='Location for parameter checkpoints and samples')\n",
    "parser.add_argument('-d', '--dataset', type=str,\n",
    "                    default='cifar', help='Can be either cifar|mnist')\n",
    "parser.add_argument('-p', '--print_every', type=int, default=50,\n",
    "                    help='how many iterations between print statements')\n",
    "parser.add_argument('-t', '--save_interval', type=int, default=10,\n",
    "                    help='Every how many epochs to write checkpoint/samples?')\n",
    "parser.add_argument('-r', '--load_params', type=str, default=None,\n",
    "                    help='Restore training from previous model checkpoint?')\n",
    "# model\n",
    "parser.add_argument('-q', '--nr_resnet', type=int, default=5,\n",
    "                    help='Number of residual blocks per stage of the model')\n",
    "parser.add_argument('-n', '--nr_filters', type=int, default=160,\n",
    "                    help='Number of filters to use across the model. Higher = larger model.')\n",
    "parser.add_argument('-m', '--nr_logistic_mix', type=int, default=10,\n",
    "                    help='Number of logistic components in the mixture. Higher = more flexible model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('-l', '--lr', type=float,\n",
    "                    default=0.0002, help='Base learning rate')\n",
    "parser.add_argument('-e', '--lr_decay', type=float, default=0.999995,\n",
    "                    help='Learning rate decay, applied every step of the optimization')\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=64,\n",
    "                    help='Batch size during training per GPU')\n",
    "parser.add_argument('-x', '--max_epochs', type=int,\n",
    "                    default=5000, help='How many epochs to run in total?')\n",
    "parser.add_argument('-s', '--seed', type=int, default=1,\n",
    "                    help='Random seed to use')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "model_name = 'pcnn_lr:{:.5f}_nr-resnet{}_nr-filters{}'.format(args.lr, args.nr_resnet, args.nr_filters)\n",
    "assert not os.path.exists(os.path.join('runs', model_name)), '{} already exists!'.format(model_name)\n",
    "writer = SummaryWriter(log_dir=os.path.join('runs', model_name))\n",
    "\n",
    "sample_batch_size = 25\n",
    "obs = (1, 28, 28) if 'mnist' in args.dataset else (3, 32, 32)\n",
    "input_channels = obs[0]  # obs[0] : channel (if color=3, if grayscale = 1)\n",
    "rescaling     = lambda x : (x - .5) * 2.\n",
    "rescaling_inv = lambda x : .5 * x  + .5\n",
    "kwargs = {'num_workers':1, 'pin_memory':True, 'drop_last':True}\n",
    "ds_transforms = transforms.Compose([transforms.ToTensor(), rescaling])\n",
    "\n",
    "#dataset은 어떤 것을 이용? [ mnist / cifar]\n",
    "if 'mnist' in args.dataset : \n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST(args.data_dir, download=True, \n",
    "                        train=True, transform=ds_transforms), batch_size=args.batch_size, \n",
    "                            shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader  = torch.utils.data.DataLoader(datasets.MNIST(args.data_dir, train=False, \n",
    "                    transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    loss_op   = lambda real, fake : discretized_mix_logistic_loss_1d(real, fake)\n",
    "    # sample_op returns sample from discretized mix logistic 1d\n",
    "    sample_op = lambda x : sample_from_discretized_mix_logistic_1d(x, args.nr_logistic_mix) \n",
    "    \n",
    "elif 'cifar' in args.dataset : \n",
    "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(args.data_dir, train=True, \n",
    "        download=True, transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader  = torch.utils.data.DataLoader(datasets.CIFAR10(args.data_dir, train=False, \n",
    "                    transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    loss_op   = lambda real, fake : discretized_mix_logistic_loss(real, fake)\n",
    "    # sample_op returns sample from discretized mix logistic 1d\n",
    "    sample_op = lambda x : sample_from_discretized_mix_logistic(x, args.nr_logistic_mix)\n",
    "else :\n",
    "    raise Exception('{} dataset not in {mnist, cifar10}'.format(args.dataset))\n",
    "\n",
    "# call model with parsed arguments    \n",
    "model = PixelCNN(nr_resnet=args.nr_resnet, nr_filters=args.nr_filters, \n",
    "            input_channels=input_channels, nr_logistic_mix=args.nr_logistic_mix)\n",
    "model = model.cuda()\n",
    "\n",
    "if args.load_params:\n",
    "    load_part_of_model(model, args.load_params)\n",
    "    # model.load_state_dict(torch.load(args.load_params))\n",
    "    print('model parameters loaded')\n",
    "\n",
    "# set optimizer, scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.lr_decay)\n",
    "\n",
    "# define sample\n",
    "def sample(model):\n",
    "    model.train(False) # don't train model\n",
    "    data = torch.zeros(sample_batch_size, obs[0], obs[1], obs[2]) # generate 0 filled tensor, \n",
    "    # which size is : (batch size, input channel, image width, image height)\n",
    "    data = data.cuda()\n",
    "    for i in range(obs[1]):\n",
    "        for j in range(obs[2]): # iterate pixels\n",
    "            data_v = Variable(data, volatile=True) \n",
    "            out   = model(data_v, sample=True)\n",
    "            out_sample = sample_op(out)\n",
    "            data[:, :, i, j] = out_sample.data[:, :, i, j]\n",
    "    return data\n",
    "\n",
    "print('starting training')\n",
    "writes = 0\n",
    "for epoch in range(args.max_epochs):\n",
    "    model.train(True)\n",
    "    torch.cuda.synchronize()\n",
    "    train_loss = 0.\n",
    "    time_ = time.time()\n",
    "    model.train()\n",
    "    for batch_idx, (input,_) in enumerate(train_loader):\n",
    "        input = input.cuda(async=True)\n",
    "        input = Variable(input)\n",
    "        output = model(input)\n",
    "        loss = loss_op(input, output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        if (batch_idx +1) % args.print_every == 0 : \n",
    "            deno = args.print_every * args.batch_size * np.prod(obs) * np.log(2.)\n",
    "            writer.add_scalar('train/bpd', (train_loss / deno), writes)\n",
    "            print('loss : {:.4f}, time : {:.4f}'.format(\n",
    "                (train_loss / deno), \n",
    "                (time.time() - time_)))\n",
    "            train_loss = 0.\n",
    "            writes += 1\n",
    "            time_ = time.time()\n",
    "            \n",
    "\n",
    "    # decrease learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    for batch_idx, (input,_) in enumerate(test_loader):\n",
    "        input = input.cuda(async=True)\n",
    "        input_var = Variable(input)\n",
    "        output = model(input_var)\n",
    "        loss = loss_op(input_var, output)\n",
    "        test_loss += loss.data[0]\n",
    "        del loss, output\n",
    "\n",
    "    deno = batch_idx * args.batch_size * np.prod(obs) * np.log(2.)\n",
    "    writer.add_scalar('test/bpd', (test_loss / deno), writes)\n",
    "    print('test loss : %s' % (test_loss / deno))\n",
    "    \n",
    "    if (epoch + 1) % args.save_interval == 0: \n",
    "        torch.save(model.state_dict(), 'models/{}_{}.pth'.format(model_name, epoch))\n",
    "        print('sampling...')\n",
    "        sample_t = sample(model)\n",
    "        sample_t = rescaling_inv(sample_t)\n",
    "        utils.save_image(sample_t,'images/{}_{}.png'.format(model_name, epoch), \n",
    "                nrow=5, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
